{"paragraphs":[{"text":"%md\n## Welcome to BDTwitrend.\n#### This is the class project of Big Data Technology course, you can run the code yourself. (Shift-Enter to Run)\n##### Batjargal Bayarsaikhan (Alex) - 986264\n##### March 14-19 2018","user":"anonymous","dateUpdated":"2018-03-16T01:53:23-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Welcome to BDTwitrend.</h2>\n<h4>This is the class project of Big Data Technology course, you can run the code yourself. (Shift-Enter to Run)</h4>\n<h5>Batjargal Bayarsaikhan (Alex) - 986264</h5>\n<h5>March 14-19 2018</h5>\n</div>"}]},"apps":[],"jobName":"paragraph_1521168859685_-851216986","id":"20180315-215419_1197411295","dateCreated":"2018-03-15T21:54:19-0500","dateStarted":"2018-03-16T01:53:23-0500","dateFinished":"2018-03-16T01:53:28-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:650"},{"title":"Load Highcharts","text":"%angular\n<script type=\"text/javascript\">\n\n\t$(function () {\n\t    if (typeof Highcharts == \"undefined\") {\n\t\t\t$.getScript(\"http://code.highcharts.com/highcharts.js\")\n\t\t\t  .done(function( script, textStatus ) {\n\t\t\t    console.log( \"load http://code.highcharts.com/highcharts.js \" + textStatus );\n\t\t\t  })\n\t\t\t  .fail(function(jqxhr, settings, exception ) {\n\t\t\t     console.log(\"load http://code.highcharts.com/highcharts.js \" + exception);\n\t\t\t  });\n\t\t} else {\n\t\t    console.log(\"highcharts already loaded\");\n\t\t}\n\t});\n</script>\n<script type=\"text/javascript\">\n\n\t$(function () {\n\t\t\t$.getScript(\"https://code.highcharts.com/modules/drilldown.js\")\n\t\t\t  .done(function( script, textStatus ) {\n\t\t\t    console.log( \"load https://code.highcharts.com/modules/drilldown.js \" + textStatus );\n\t\t\t  })\n\t\t\t  .fail(function(jqxhr, settings, exception ) {\n\t\t\t     console.log(\"load https://code.highcharts.com/modules/drilldown.js \" + exception);\n\t\t\t  });\n\t});\n</script>\n","user":"anonymous","dateUpdated":"2018-03-17T02:49:02-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<script type=\"text/javascript\">\n\n\t$(function () {\n\t    if (typeof Highcharts == \"undefined\") {\n\t\t\t$.getScript(\"http://code.highcharts.com/highcharts.js\")\n\t\t\t  .done(function( script, textStatus ) {\n\t\t\t    console.log( \"load http://code.highcharts.com/highcharts.js \" + textStatus );\n\t\t\t  })\n\t\t\t  .fail(function(jqxhr, settings, exception ) {\n\t\t\t     console.log(\"load http://code.highcharts.com/highcharts.js \" + exception);\n\t\t\t  });\n\t\t} else {\n\t\t    console.log(\"highcharts already loaded\");\n\t\t}\n\t});\n</script>\n<script type=\"text/javascript\">\n\n\t$(function () {\n\t\t\t$.getScript(\"https://code.highcharts.com/modules/drilldown.js\")\n\t\t\t  .done(function( script, textStatus ) {\n\t\t\t    console.log( \"load https://code.highcharts.com/modules/drilldown.js \" + textStatus );\n\t\t\t  })\n\t\t\t  .fail(function(jqxhr, settings, exception ) {\n\t\t\t     console.log(\"load https://code.highcharts.com/modules/drilldown.js \" + exception);\n\t\t\t  });\n\t});\n</script>"}]},"apps":[],"jobName":"paragraph_1521272791221_657480420","id":"20180317-024631_761596839","dateCreated":"2018-03-17T02:46:31-0500","dateStarted":"2018-03-17T02:49:02-0500","dateFinished":"2018-03-17T02:49:02-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:651"},{"title":"Load data into table (Spark/HBase/DataFrames)","text":"%spark\n\nimport org.apache.spark.sql.execution.datasources.hbase._\nimport org.apache.spark.sql._\n\ndef catalogTweet = s\"\"\"{\n  |\"table\":{\"namespace\":\"default\", \"name\":\"tblTweet\"},\n  |\"rowkey\":\"key\",\n  |\"columns\":{\n    |\"key\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n    |\"tweet\":{\"cf\":\"Tweet\", \"col\":\"Text\", \"type\":\"string\"},\n    |\"hashtags\":{\"cf\":\"Tweet\", \"col\":\"Hashtags\", \"type\":\"string\"},\n    |\"created\":{\"cf\":\"Tweet\", \"col\":\"Created\", \"type\":\"string\"},\n    |\"user\":{\"cf\":\"Tweet\", \"col\":\"User\", \"type\":\"string\"},\n    |\"latitude\":{\"cf\":\"Geo\", \"col\":\"Latitude\", \"type\":\"string\"},\n    |\"longitude\":{\"cf\":\"Geo\", \"col\":\"Longitude\", \"type\":\"string\"}\n  |}\n|}\"\"\".stripMargin\n\n\ndef catalogHashTag = s\"\"\"{\n  |\"table\":{\"namespace\":\"default\", \"name\":\"tblHashTag\"},\n  |\"rowkey\":\"key\",\n  |\"columns\":{\n    |\"key\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n    |\"hashtag\":{\"cf\":\"Info\", \"col\":\"HashTag\", \"type\":\"string\"},\n    |\"created\":{\"cf\":\"Info\", \"col\":\"Created\", \"type\":\"date\"}\n  |}\n|}\"\"\".stripMargin\n\ndef withCatalog(cat: String): DataFrame = {\n  sqlContext\n  .read\n  .options(Map(HBaseTableCatalog.tableCatalog->cat))\n  .format(\"org.apache.spark.sql.execution.datasources.hbase\")\n  .load()\n}\n\nval dfTweet = withCatalog(catalogTweet).select(\"key\", \"tweet\", \"hashtags\", \"created\", \"user\", \"latitude\", \"longitude\")\nval dfHashTag = withCatalog(catalogHashTag).select(\"key\", \"hashtag\", \"created\")\n\ndfTweet.registerTempTable(\"tblTweet\")\ndfHashTag.registerTempTable(\"tblHashTag\")\n\n//sqlContext.sql(\"select count(col0) from tblTweetHashTag\").show\n\n    ","user":"anonymous","dateUpdated":"2018-03-17T22:07:29-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true,"title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.execution.datasources.hbase._\nimport org.apache.spark.sql._\ncatalogTweet: String\ncatalogHashTag: String\nwithCatalog: (cat: String)org.apache.spark.sql.DataFrame\ndfTweet: org.apache.spark.sql.DataFrame = [key: string, tweet: string ... 5 more fields]\ndfHashTag: org.apache.spark.sql.DataFrame = [key: string, hashtag: string ... 1 more field]\nwarning: there was one deprecation warning; re-run with -deprecation for details\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"}]},"apps":[],"jobName":"paragraph_1521173966188_1155987074","id":"20180315-231926_1177327453","dateCreated":"2018-03-15T23:19:26-0500","dateStarted":"2018-03-17T20:48:29-0500","dateFinished":"2018-03-17T20:48:42-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:652"},{"title":"Top 10 topics","text":"%sql\n\nselect hashtag, count(hashtag) as total from tblHashTag group by hashtag order by total desc limit 10","user":"anonymous","dateUpdated":"2018-03-17T21:58:21-0500","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"hashtag\ttotal\nKCA\t9\nGioXJoanneOnASAPSummerTrip\t8\nEXO\t7\nQueenKathrynShinesAt22\t6\nElyXioninBKK\t6\nNowPlaying\t5\nGagaBestFans\t5\n엑소\t5\nKTBFFH\t5\nZaraBestFans\t4\n"}]},"apps":[],"jobName":"paragraph_1521335710222_2119324792","id":"20180317-201510_1241575138","dateCreated":"2018-03-17T20:15:10-0500","dateStarted":"2018-03-17T21:58:21-0500","dateFinished":"2018-03-17T21:58:22-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:654"},{"title":"Top 10 topics","text":"%sql\nselect HashTag, count(hashtag) as total from tblHashTag group by hashtag order by total desc limit 10","user":"anonymous","dateUpdated":"2018-03-17T22:20:30-0500","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"HashTag\ttotal\nGioXJoanneOnASAPSummerTrip\t16\nKCA\t14\nEXO\t13\n엑소\t11\nQueenKathrynShinesAt22\t10\nStPatricksDay\t9\nElyXioninBKK\t9\nGagaBestFans\t8\nNowPlaying\t8\nZaraBestFans\t7\n"}]},"apps":[],"jobName":"paragraph_1521335706434_621882073","id":"20180317-201506_2123282838","dateCreated":"2018-03-17T20:15:06-0500","dateStarted":"2018-03-17T22:20:30-0500","dateFinished":"2018-03-17T22:20:31-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:655"},{"title":"Most active 10 users","text":"%sql\nselect User, count(user) as NumberOfTweets from tblTweet group by user order by NumberOfTweets desc limit 10\n","user":"anonymous","dateUpdated":"2018-03-17T22:07:02-0500","config":{"colWidth":4,"enabled":true,"results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"User\tNumberOfTweets\n.\t5\ngabby\t3\n☀️\t2\nTili Sokolov 🌹\t2\nTara Simmons\t2\nAmanda\t2\nsarah\t2\nJason\t2\nK\t2\n;\t2\n"}]},"apps":[],"jobName":"paragraph_1521337847889_-1105975452","id":"20180317-205047_1671066571","dateCreated":"2018-03-17T20:50:47-0500","dateStarted":"2018-03-17T21:57:57-0500","dateFinished":"2018-03-17T21:57:59-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:656"},{"title":"Most active 10 users","text":"%sql\nselect User, count(user) as NumberOfTweets from tblTweet group by user order by NumberOfTweets desc limit 10","user":"anonymous","dateUpdated":"2018-03-17T22:07:05-0500","config":{"colWidth":4,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"User\tNumberOfTweets\n.\t7\nkayla\t3\nM\t3\nMichael\t3\ngabby\t3\nJasmine\t3\nAmanda\t2\nDenise\t2\nTili Sokolov 🌹\t2\nJason\t2\n"}]},"apps":[],"jobName":"paragraph_1521338205539_-1629869349","id":"20180317-205645_2090973195","dateCreated":"2018-03-17T20:56:45-0500","dateStarted":"2018-03-17T21:58:52-0500","dateFinished":"2018-03-17T21:58:53-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:657"},{"title":"Last 10 users","text":"%sql\nselect distinct User, date_format(created, \"yyyy-MM-dd HH:mm:ss\") as Created from tblTweet order by Created desc  limit 10","user":"anonymous","dateUpdated":"2018-03-17T23:18:29-0500","config":{"colWidth":4,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"User\tCreated\nSharvin\t2018-03-17 21:52:17\nsusan christensen\t2018-03-17 21:51:00\nmarilu callahan\t2018-03-17 21:50:44\nvinnie\t2018-03-17 21:50:08\nمنيب\t2018-03-17 21:46:22\nkatharyn wilsoniii\t2018-03-17 21:41:56\nJ Breezy\t2018-03-17 21:40:48\nmeghan soulignac\t2018-03-17 21:37:36\nlatrice kim\t2018-03-17 21:37:04\naracely grimes\t2018-03-17 21:35:21\n"}]},"apps":[],"jobName":"paragraph_1521338314680_-1062243772","id":"20180317-205834_857188744","dateCreated":"2018-03-17T20:58:34-0500","dateStarted":"2018-03-17T23:18:25-0500","dateFinished":"2018-03-17T23:18:27-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:658"},{"title":"Last 10 tweets","text":"%sql\nselect   regexp_replace(tweet, '[^a-zA-Z0-9 !,@#$%^&*\\(\\)\\-\\+\\\\:;\\'/<>\\?]+', \"\") as Tweet, User, date_format(created, \"yyyy-MM-dd HH:mm:ss\") as Created from tblTweet order by created desc limit 10\n","user":"anonymous","dateUpdated":"2018-03-17T23:16:56-0500","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql","title":true,"tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Tweet\tUser\tCreated\nRT @immancomposer: Hearty Birthday wishes to Singing Princess! @shreyaghoshal ! Enlighten this world with more of your musical gems across\tSharvin\t2018-03-17 21:52:17\nRT @MarkBeech: A band called The Dancing Did (A did is a gypsy) The name, and origins in Evesham, where I grew up, https://tco/bYn\tsusan christensen\t2018-03-17 21:51:00\nRT @milancita305: * Latinos and those who love them, Tonight is Our Night!!!BAILO at TWELVE 10:30PM7335 NW 36th StreetMiami, FL 33166N\tmarilu callahan\t2018-03-17 21:50:44\nRT @KPKelly: Loving who you are, truly loving yourself, is a requirement for true peace and joy https://tco/d2nNEXXW9f\tvinnie\t2018-03-17 21:50:08\nRT @firedonniedanny: I keep my timeline clean on account of all the stupid kids\tمنيب\t2018-03-17 21:46:22\nRT @ShujaRabbani: Happy days of @nytimes seem to be numbered https://tco/uzJmZB3ZSw\tkatharyn wilsoniii\t2018-03-17 21:41:56\nHoltmann has shown he can do it Let's see these recruits come in and hopefully players develop to soften the blow of batesdiop\tJ Breezy\t2018-03-17 21:40:48\nRT @MarkBeech: This lovely song still makes me smile @PaoloNutini https://tco/h3Tuw3PQro\tmeghan soulignac\t2018-03-17 21:37:36\nRT @TheRiseofNazil: New Release! Kurintor Nyusi is available now Welcome to the Fifth Kingdom!https://tco/ZPfxYFCCP6#Fantasy #Divers\tlatrice kim\t2018-03-17 21:37:04\nRT @RealMarkusR: The poorest man is not the one without a penny, the poorest man is the one without a dream\taracely grimes\t2018-03-17 21:35:21\n"}]},"apps":[],"jobName":"paragraph_1521342481083_-175716903","id":"20180317-220801_1791096802","dateCreated":"2018-03-17T22:08:01-0500","dateStarted":"2018-03-17T23:15:30-0500","dateFinished":"2018-03-17T23:15:31-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:653","focus":true},{"text":"%spark\nimport org.apache.spark.SparkConf\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network every second.\n *\n * Usage: NetworkWordCount <hostname> <port>\n * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999`\n */\n\n\n// prevent INFO logging from pollution output\nsc.setLogLevel(\"ERROR\")\n\n    // Create the context with a 1 second batch size\n    //val sparkConf = new SparkConf().setAppName(\"NetworkWordCount\")\n    val ssc = new StreamingContext(sc, Seconds(10))\n\n    // Create a socket stream on target ip:port and count the\n    // words in input stream of \\n delimited text (eg. generated by 'nc')\n    // Note that no duplication in storage level only for running locally.\n    // Replication necessary in distributed scenario for fault tolerance.\n    val lines = ssc.socketTextStream(\"localhost\", 9092, StorageLevel.MEMORY_AND_DISK_SER)\n    val words = lines.flatMap(_.split(\" \"))\n    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)\n    wordCounts.print()\n    ssc.start()\n    ssc.awaitTermination()","user":"anonymous","dateUpdated":"2018-03-17T22:07:14-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521314815798_1081747825","id":"20180317-142655_894550980","dateCreated":"2018-03-17T14:26:55-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:659"},{"text":"%spark\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.storage.StorageLevel\nimport scala.io.Source\nimport scala.collection.mutable.HashMap\nimport java.io.File\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport sys.process.stringSeqToProcess\n\n/** Configures the Oauth Credentials for accessing Twitter */\ndef configureTwitterCredentials(apiKey: String, apiSecret: String, accessToken: String, accessTokenSecret: String) {\n  val configs = new HashMap[String, String] ++= Seq(\n    \"apiKey\" -> apiKey, \"apiSecret\" -> apiSecret, \"accessToken\" -> accessToken, \"accessTokenSecret\" -> accessTokenSecret)\n  println(\"Configuring Twitter OAuth\")\n  configs.foreach{ case(key, value) =>\n    if (value.trim.isEmpty) {\n      throw new Exception(\"Error setting authentication - value for \" + key + \" not set\")\n    }\n    val fullKey = \"twitter4j.oauth.\" + key.replace(\"api\", \"consumer\")\n    System.setProperty(fullKey, value.trim)\n    println(\"\\tProperty \" + fullKey + \" set as [\" + value.trim + \"]\")\n  }\n  println()\n}\n\n\n// Configure Twitter credentials\nval apiKey = \"4dItFxuN84A2tRu8lsFZxkxQb\"\nval apiSecret = \"1z3EhIOEhq3kNhbBR7jM439qqMDxZX0QY1RE9u2XUo3TC7otI5\"\nval accessToken = \"1104333505-p1hkk15wYzKmK4SHlfMIwrQXOceBKuVgAKm3h6z\"\nval accessTokenSecret = \"prCMYT4O3nbEjZNw4yFo5fYwWzdyG3NwwoASC13Apw5yD\"\nconfigureTwitterCredentials(apiKey, apiSecret, accessToken, accessTokenSecret)\n\nimport org.apache.spark.streaming.twitter._\nval ssc = new StreamingContext(sc, Seconds(2))\nval tweets = TwitterUtils.createStream(ssc, None)\nval twt = tweets.window(Seconds(60))\n\ncase class Tweet(createdAt:Long, text:String)\ntwt.map(status=>\n  Tweet(status.getCreatedAt().getTime()/1000, status.getText())\n).foreachRDD(rdd=>\n  // Below line works only in spark 1.3.0.\n  // For spark 1.1.x and spark 1.2.x,\n  // use rdd.registerTempTable(\"tweets\") instead.\n  rdd.toDF().registerAsTable(\"tweets\")\n)\n\ntwt.print\n\nssc.start()\n","user":"anonymous","dateUpdated":"2018-03-17T20:14:04-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.streaming._\n<console>:28: error: object twitter is not a member of package org.apache.spark.streaming\n       import org.apache.spark.streaming.twitter._\n                                         ^\n"}]},"apps":[],"jobName":"paragraph_1521318562359_1432048804","id":"20180317-152922_176413850","dateCreated":"2018-03-17T15:29:22-0500","dateStarted":"2018-03-17T15:44:15-0500","dateFinished":"2018-03-17T15:44:44-0500","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:660"},{"text":"%spark\nimport com.knockdata.spark.highcharts._\nimport com.knockdata.spark.highcharts.model._\nimport org.apache.spark.sql.SparkSession\nval spark = SparkSession\n  .builder()\n  .appName(\"Spark structured streaming Kafka example\")\n  .master(\"yarn\")\n  .getOrCreate()\n\nval inputstream = spark.readStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n    .option(\"subscribe\", \"streams-plaintext-input\")\n    .load()\n    spark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"checkpoint\")\n\nval ValueString = inputstream.selectExpr(\"CAST( value AS STRING)\").as[(String)]\n.select(\n\n                    expr(\"(split(value, ','))[0]\").cast(\"string\").as(\"HashTag\"),\n                    expr(\"(split(value, ','))[1]\").cast(\"double\").as(\"Duration\")\n                    )\n                    .groupBy(\"HashTag\").agg(sum(\"Duration\") as \"duration\")\nval query = highcharts(\nValueString.seriesCol(\"HashTag\")\n.series(\"y\" -> \"duration\",\"x\" -> \"duration\")\n.orderBy(col(\"HashTag\")), z, \"complete\")\nquery.processAllAvailable()\nquery.awaitTermination()\n","user":"anonymous","dateUpdated":"2018-03-17T20:14:00-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import com.knockdata.spark.highcharts._\nimport com.knockdata.spark.highcharts.model._\nimport org.apache.spark.sql.SparkSession\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@568a348a\njava.lang.ClassNotFoundException: Failed to find data source: kafka. Please find packages at http://spark.apache.org/third-party-projects.html\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:569)\n  at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:197)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:87)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:87)\n  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:124)\n  ... 50 elided\nCaused by: java.lang.ClassNotFoundException: kafka.DefaultSource\n  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25$$anonfun$apply$13.apply(DataSource.scala:554)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25$$anonfun$apply$13.apply(DataSource.scala:554)\n  at scala.util.Try$.apply(Try.scala:192)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25.apply(DataSource.scala:554)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25.apply(DataSource.scala:554)\n  at scala.util.Try.orElse(Try.scala:84)\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:554)\n  ... 57 more\n"}]},"apps":[],"jobName":"paragraph_1521313202704_2034315535","id":"20180317-140002_758561392","dateCreated":"2018-03-17T14:00:02-0500","dateStarted":"2018-03-17T15:08:14-0500","dateFinished":"2018-03-17T15:08:17-0500","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:661"},{"text":"%spark\n\nimport com.knockdata.spark.highcharts._\nimport com.knockdata.spark.highcharts.model.{Chart, Highcharts, Series, Tooltip, XAxis, YAxis}\nimport org.apache.spark.sql.functions._\n\n    val chart = highcharts(s\n      .series(\"x\" -> \"col1\", \"y\" -> count(\"col2\"))\n      .orderBy(col(\"col1\")))\n      .chart(Chart.bar)\n      .plotOptions(PlotOptions.column.groupPadding(0).pointPadding(0).borderWidth(0))\n\n    chart.plot()\n    \n\n    val chart1 = new Highcharts(List(seriesBrowser, seriesVersion))\n      .chart(Chart.pie)\n      \n          val chart2 = highcharts(s.series(\n      \"name\" -> \"col1\",\n      \"y\" -> sum(col(\"col2\")),\n      \"orderBy\" -> col(\"col1\")))\n      .chart(Chart.pie)\n      //.size(\"60%\")\n      //.dataLabels(\n    //    \"distance\" -> -30,\n      //  \"formatter\" ->\n    //      \"\"\"\n      //      |function() {\n        ///    |  return this.y > 1 ? this.point.name : null;\n           // |}\n          //\"\"\".stripMargin)\n    chart2.plot()\n    //chart2.html\n\n    //chart1.plot()","user":"anonymous","dateUpdated":"2018-03-17T16:49:44-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.knockdata.spark.highcharts._\nimport com.knockdata.spark.highcharts.model.{Chart, Highcharts, Series, Tooltip, XAxis, YAxis}\nimport org.apache.spark.sql.functions._\nchart: com.knockdata.spark.highcharts.model.Highcharts = com.knockdata.spark.highcharts.model.Highcharts@4cdb9638\n"},{"type":"ANGULAR","data":"\n<div id=\"highcharts_9ebedaea_6122_4fc4_8099_1c66172c822c\" style=\"min-width: 310px; height: 400px; margin: 0 auto\"></div>\n\n<script type=\"text/javascript\">\n$(function () {\nvar data = {\n  \"series\":[{\n    \"data\":[{\n      \"x\":\"are\",\n      \"y\":1\n    },{\n      \"x\":\"command\",\n      \"y\":1\n    },{\n      \"x\":\"gme\",\n      \"y\":1\n    },{\n      \"x\":\"hello\",\n      \"y\":1\n    },{\n      \"x\":\"how\",\n      \"y\":1\n    },{\n      \"x\":\"interesting\",\n      \"y\":1\n    },{\n      \"x\":\"is\",\n      \"y\":1\n    },{\n      \"x\":\"it,\",\n      \"y\":1\n    },{\n      \"x\":\"line\",\n      \"y\":1\n    },{\n      \"x\":\"not\",\n      \"y\":1\n    },{\n      \"x\":\"think\",\n      \"y\":1\n    },{\n      \"x\":\"working\",\n      \"y\":1\n    },{\n      \"x\":\"world\",\n      \"y\":1\n    },{\n      \"x\":\"you\",\n      \"y\":1\n    }]\n  }],\n  \"chart\":{\n    \"type\":\"bar\"\n  },\n  \"plotOptions\":{\n    \"column\":{\n      \"groupPadding\":0,\n      \"pointPadding\":0,\n      \"borderWidth\":0\n    }\n  }\n}\n\n$(\"#highcharts_9ebedaea_6122_4fc4_8099_1c66172c822c\").highcharts(data)\n});\n</script>\nchart1: com.knockdata.spark.highcharts.model.Highcharts = com.knockdata.spark.highcharts.model.Highcharts@3a5cc98e\nchart2: com.knockdata.spark.highcharts.model.Highcharts = com.knockdata.spark.highcharts.model.Highcharts@1785ee98\n"},{"type":"ANGULAR","data":"\n<div id=\"highcharts_bf3bfe67_0bb3_4ce5_9ec0_b46895aa1fa6\" style=\"min-width: 310px; height: 400px; margin: 0 auto\"></div>\n\n<script type=\"text/javascript\">\n$(function () {\nvar data = {\n  \"series\":[{\n    \"data\":[{\n      \"name\":\"not\",\n      \"y\":1,\n      \"orderBy\":\"not\"\n    },{\n      \"name\":\"you\",\n      \"y\":2,\n      \"orderBy\":\"you\"\n    },{\n      \"name\":\"how\",\n      \"y\":3,\n      \"orderBy\":\"how\"\n    },{\n      \"name\":\"hello\",\n      \"y\":2,\n      \"orderBy\":\"hello\"\n    },{\n      \"name\":\"is\",\n      \"y\":2,\n      \"orderBy\":\"is\"\n    },{\n      \"name\":\"gme\",\n      \"y\":1,\n      \"orderBy\":\"gme\"\n    },{\n      \"name\":\"command\",\n      \"y\":1,\n      \"orderBy\":\"command\"\n    },{\n      \"name\":\"are\",\n      \"y\":1,\n      \"orderBy\":\"are\"\n    },{\n      \"name\":\"world\",\n      \"y\":3,\n      \"orderBy\":\"world\"\n    },{\n      \"name\":\"it,\",\n      \"y\":1,\n      \"orderBy\":\"it,\"\n    },{\n      \"name\":\"think\",\n      \"y\":1,\n      \"orderBy\":\"think\"\n    },{\n      \"name\":\"interesting\",\n      \"y\":1,\n      \"orderBy\":\"interesting\"\n    },{\n      \"name\":\"working\",\n      \"y\":1,\n      \"orderBy\":\"working\"\n    },{\n      \"name\":\"line\",\n      \"y\":8,\n      \"orderBy\":\"line\"\n    }]\n  }],\n  \"chart\":{\n    \"type\":\"pie\"\n  }\n}\n\n$(\"#highcharts_bf3bfe67_0bb3_4ce5_9ec0_b46895aa1fa6\").highcharts(data)\n});\n</script>\n"}]},"apps":[],"jobName":"paragraph_1521321030311_1774550825","id":"20180317-161030_798095889","dateCreated":"2018-03-17T16:10:30-0500","dateStarted":"2018-03-17T16:49:44-0500","dateFinished":"2018-03-17T16:50:05-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:662"},{"text":"%spark\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport com.knockdata.spark.highcharts.{CustomSinkProvider, _}\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nimport java.nio.file.Files\n\nimport org.apache.spark.sql.SparkSession\n\nobject SparkEnv {\n\n\n  //  lazy val sc = {\n  //        val master = \"local\"\n  ////    val master = \"spark://Rockies-MacBook-Pro.local:7077\"\n  //    val conf = new SparkConf()\n  //      .setAppName(\"Simple Application\")\n  //      .setMaster(master)\n  ////      .setJars(Seq(\"/Users/rockieyang/git/spark-highcharts/target/spark-highcharts-0.6.1.jar\"))\n  //    new SparkContext(conf)\n  //\n  //\n  //  }\n\n    val checkpointPath = Files.createTempDirectory(\"query\")\n    val checkpointDir = checkpointPath.toFile\n\n    checkpointDir.deleteOnExit()\n\n    def deleteRecursively(file: java.io.File): Unit = {\n      if (file.isDirectory) {\n        file.listFiles().foreach(deleteRecursively)\n        file.delete()\n      }\n      else\n        file.delete()\n    }\n\n    def clearCheckpointDir: Unit = {\n      checkpointDir.listFiles().foreach(deleteRecursively)\n    }\n\n    lazy val spark = SparkSession\n      .builder()\n      .appName(\"Spark SQL basic example\")\n      .config(\"spark.sql.streaming.checkpointLocation\",\n        checkpointDir.getAbsolutePath)\n      .master(\"local[*]\")\n      .appName(\"test\")\n      .getOrCreate()\n\n  lazy val sc = spark.sparkContext\n\n  //  val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n  lazy val sqlContext = spark.sqlContext\n\n\n  //  def createDF(seq: Seq[Any]) = {\n  //    sqlContext.createDataFrame(seq)\n  //  }\n\n}\n\n  implicit val sqlContext = SparkEnv.sqlContext\n  import SparkEnv.spark.implicits._\n\nspark.conf.set(\"spark.sql.streaming.checkpointLocation\",\"/home/bb/Document/zeppelin/checkpoint\")\n\ncase class NuclearStockpile(country: String, stockpile: Int, year: Int)\n\n    val input = MemoryStream[NuclearStockpile]\nval USA = Seq(0, 0, 0, 0, 0, 6, 11, 32, 110, 235, 369, 640,\n  1005, 1436, 2063, 3057, 4618, 6444, 9822, 15468, 20434, 24126,\n  27387, 29459, 31056, 31982, 32040, 31233, 29224, 27342, 26662,\n  26956, 27912, 28999, 28965, 27826, 25579, 25722, 24826, 24605,\n  24304, 23464, 23708, 24099, 24357, 24237, 24401, 24344, 23586,\n  22380, 21004, 17287, 14747, 13076, 12555, 12144, 11009, 10950,\n  10871, 10824, 10577, 10527, 10475, 10421, 10358, 10295, 10104).\n    zip(1940 to 2006).map(p => NuclearStockpile(\"USA\", p._1, p._2))\n\nval USSR = Seq(0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n  5, 25, 50, 120, 150, 200, 426, 660, 869, 1060, 1605, 2471, 3322,\n  4238, 5221, 6129, 7089, 8339, 9399, 10538, 11643, 13092, 14478,\n  15915, 17385, 19055, 21205, 23044, 25393, 27935, 30062, 32049,\n  33952, 35804, 37431, 39197, 45000, 43000, 41000, 39000, 37000,\n  35000, 33000, 31000, 29000, 27000, 25000, 24000, 23000, 22000,\n  21000, 20000, 19000, 18000, 18000, 17000, 16000).\n    zip(1940 to 2006).map(p => NuclearStockpile(\"USSR/Russia\", p._1, p._2))\n\ninput.addData(USA.take(30) ++ USSR.take(30))\nval structuredDataFrame = input.toDF\n\n","user":"anonymous","dateUpdated":"2018-03-17T16:02:18-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.execution.streaming.MemoryStream\nimport com.knockdata.spark.highcharts.{CustomSinkProvider, _}\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nimport java.nio.file.Files\nimport org.apache.spark.sql.SparkSession\ndefined object SparkEnv\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@671398e9\nimport SparkEnv.spark.implicits._\ndefined class NuclearStockpile\ninput: org.apache.spark.sql.execution.streaming.MemoryStream[NuclearStockpile] = MemoryStream[country#31,stockpile#32,year#33]\nUSA: Seq[NuclearStockpile] = List(NuclearStockpile(USA,0,1940), NuclearStockpile(USA,0,1941), NuclearStockpile(USA,0,1942), NuclearStockpile(USA,0,1943), NuclearStockpile(USA,0,1944), NuclearStockpile(USA,6,1945), NuclearStockpile(USA,11,1946), NuclearStockpile(USA,32,1947), NuclearStockpile(USA,110,1948), NuclearStockpile(USA,235,1949), NuclearStockpile(USA,369,1950), NuclearStockpile(USA,640,1951), NuclearStockpile(USA,1005,1952), NuclearStockpile(USA,1436,1953), NuclearStockpile(USA,2063,1954), NuclearStockpile(USA,3057,1955), NuclearStockpile(USA,4618,1956), NuclearStockpile(USA,6444,1957), NuclearStockpile(USA,9822,1958), NuclearStockpile(USA,15468,1959), NuclearStockpile(USA,20434,1960), NuclearStockpile(USA,24126,1961), NuclearStockpile(USA,27387,1962), NuclearStockpile(USA,29459...USSR: Seq[NuclearStockpile] = List(NuclearStockpile(USSR/Russia,0,1940), NuclearStockpile(USSR/Russia,0,1941), NuclearStockpile(USSR/Russia,0,1942), NuclearStockpile(USSR/Russia,0,1943), NuclearStockpile(USSR/Russia,0,1944), NuclearStockpile(USSR/Russia,0,1945), NuclearStockpile(USSR/Russia,0,1946), NuclearStockpile(USSR/Russia,0,1947), NuclearStockpile(USSR/Russia,0,1948), NuclearStockpile(USSR/Russia,0,1949), NuclearStockpile(USSR/Russia,5,1950), NuclearStockpile(USSR/Russia,25,1951), NuclearStockpile(USSR/Russia,50,1952), NuclearStockpile(USSR/Russia,120,1953), NuclearStockpile(USSR/Russia,150,1954), NuclearStockpile(USSR/Russia,200,1955), NuclearStockpile(USSR/Russia,426,1956), NuclearStockpile(USSR/Russia,660,1957), NuclearStockpile(USSR/Russia,869,1958), NuclearStockpile(USSR/Russ...res138: org.apache.spark.sql.execution.streaming.Offset = 0\nstructuredDataFrame: org.apache.spark.sql.DataFrame = [country: string, stockpile: int ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1521275071710_1317326722","id":"20180317-032431_1547867138","dateCreated":"2018-03-17T03:24:31-0500","dateStarted":"2018-03-17T03:40:05-0500","dateFinished":"2018-03-17T03:40:15-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:663"},{"text":"%spark\nimport com.knockdata.spark.highcharts._\nimport com.knockdata.spark.highcharts.model._\n\nval query = highcharts(\n  structuredDataFrame.seriesCol(\"country\")\n    .series(\"x\" -> \"year\", \"y\" -> \"stockpile\")\n    .orderBy(col(\"year\")), z, \"append\")","user":"anonymous","dateUpdated":"2018-03-17T16:16:23-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.knockdata.spark.highcharts._\nimport com.knockdata.spark.highcharts.model._\nquery: org.apache.spark.sql.streaming.StreamingQuery = Streaming Query [id = 206d0c57-41f5-476a-8b68-37cdaf6dc4b9, runId = 7bd15fdb-ff3f-479a-86b7-dde63334c2a6] [state = ACTIVE]\n"}]},"apps":[],"jobName":"paragraph_1521275015901_-295833471","id":"20180317-032335_1520972436","dateCreated":"2018-03-17T03:23:35-0500","dateStarted":"2018-03-17T03:40:20-0500","dateFinished":"2018-03-17T03:40:25-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:664"},{"text":"%spark\nStreamingChart(z)","user":"anonymous","dateUpdated":"2018-03-17T16:16:39-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"\n<div id=\"highcharts_fd2d2ee7_06dd_43a4_a45c_724c178f7d8d\" style=\"min-width: 310px; height: 400px; margin: 0 auto\"></div>\n\n<script type=\"text/javascript\">\n$(function () {\nvar data = {\n  \"series\":[{\n    \"id\":\"country=USSR/Russia\",\n    \"name\":\"USSR/Russia\",\n    \"data\":[{\n      \"x\":1970,\n      \"y\":11643\n    },{\n      \"x\":1971,\n      \"y\":13092\n    },{\n      \"x\":1972,\n      \"y\":14478\n    },{\n      \"x\":1973,\n      \"y\":15915\n    },{\n      \"x\":1974,\n      \"y\":17385\n    },{\n      \"x\":1975,\n      \"y\":19055\n    },{\n      \"x\":1976,\n      \"y\":21205\n    },{\n      \"x\":1977,\n      \"y\":23044\n    },{\n      \"x\":1978,\n      \"y\":25393\n    },{\n      \"x\":1979,\n      \"y\":27935\n    },{\n      \"x\":1980,\n      \"y\":30062\n    },{\n      \"x\":1981,\n      \"y\":32049\n    },{\n      \"x\":1982,\n      \"y\":33952\n    },{\n      \"x\":1983,\n      \"y\":35804\n    },{\n      \"x\":1984,\n      \"y\":37431\n    },{\n      \"x\":1985,\n      \"y\":39197\n    },{\n      \"x\":1986,\n      \"y\":45000\n    },{\n      \"x\":1987,\n      \"y\":43000\n    },{\n      \"x\":1988,\n      \"y\":41000\n    },{\n      \"x\":1989,\n      \"y\":39000\n    },{\n      \"x\":1990,\n      \"y\":37000\n    },{\n      \"x\":1991,\n      \"y\":35000\n    },{\n      \"x\":1992,\n      \"y\":33000\n    },{\n      \"x\":1993,\n      \"y\":31000\n    },{\n      \"x\":1994,\n      \"y\":29000\n    },{\n      \"x\":1995,\n      \"y\":27000\n    },{\n      \"x\":1996,\n      \"y\":25000\n    },{\n      \"x\":1997,\n      \"y\":24000\n    },{\n      \"x\":1998,\n      \"y\":23000\n    },{\n      \"x\":1999,\n      \"y\":22000\n    },{\n      \"x\":2000,\n      \"y\":21000\n    },{\n      \"x\":2001,\n      \"y\":20000\n    },{\n      \"x\":2002,\n      \"y\":19000\n    },{\n      \"x\":2003,\n      \"y\":18000\n    },{\n      \"x\":2004,\n      \"y\":18000\n    },{\n      \"x\":2005,\n      \"y\":17000\n    },{\n      \"x\":2006,\n      \"y\":16000\n    },{\n      \"x\":1970,\n      \"y\":11643\n    },{\n      \"x\":1971,\n      \"y\":13092\n    },{\n      \"x\":1972,\n      \"y\":14478\n    },{\n      \"x\":1973,\n      \"y\":15915\n    },{\n      \"x\":1974,\n      \"y\":17385\n    },{\n      \"x\":1975,\n      \"y\":19055\n    },{\n      \"x\":1976,\n      \"y\":21205\n    },{\n      \"x\":1977,\n      \"y\":23044\n    },{\n      \"x\":1978,\n      \"y\":25393\n    },{\n      \"x\":1979,\n      \"y\":27935\n    },{\n      \"x\":1980,\n      \"y\":30062\n    },{\n      \"x\":1981,\n      \"y\":32049\n    },{\n      \"x\":1982,\n      \"y\":33952\n    },{\n      \"x\":1983,\n      \"y\":35804\n    },{\n      \"x\":1984,\n      \"y\":37431\n    },{\n      \"x\":1985,\n      \"y\":39197\n    },{\n      \"x\":1986,\n      \"y\":45000\n    },{\n      \"x\":1987,\n      \"y\":43000\n    },{\n      \"x\":1988,\n      \"y\":41000\n    },{\n      \"x\":1989,\n      \"y\":39000\n    },{\n      \"x\":1990,\n      \"y\":37000\n    },{\n      \"x\":1991,\n      \"y\":35000\n    },{\n      \"x\":1992,\n      \"y\":33000\n    },{\n      \"x\":1993,\n      \"y\":31000\n    },{\n      \"x\":1994,\n      \"y\":29000\n    },{\n      \"x\":1995,\n      \"y\":27000\n    },{\n      \"x\":1996,\n      \"y\":25000\n    },{\n      \"x\":1997,\n      \"y\":24000\n    },{\n      \"x\":1998,\n      \"y\":23000\n    },{\n      \"x\":1999,\n      \"y\":22000\n    },{\n      \"x\":2000,\n      \"y\":21000\n    },{\n      \"x\":2001,\n      \"y\":20000\n    },{\n      \"x\":2002,\n      \"y\":19000\n    },{\n      \"x\":2003,\n      \"y\":18000\n    },{\n      \"x\":2004,\n      \"y\":18000\n    },{\n      \"x\":2005,\n      \"y\":17000\n    },{\n      \"x\":2006,\n      \"y\":16000\n    }]\n  },{\n    \"id\":\"country=USA\",\n    \"name\":\"USA\",\n    \"data\":[{\n      \"x\":1970,\n      \"y\":26662\n    },{\n      \"x\":1971,\n      \"y\":26956\n    },{\n      \"x\":1972,\n      \"y\":27912\n    },{\n      \"x\":1973,\n      \"y\":28999\n    },{\n      \"x\":1974,\n      \"y\":28965\n    },{\n      \"x\":1975,\n      \"y\":27826\n    },{\n      \"x\":1976,\n      \"y\":25579\n    },{\n      \"x\":1977,\n      \"y\":25722\n    },{\n      \"x\":1978,\n      \"y\":24826\n    },{\n      \"x\":1979,\n      \"y\":24605\n    },{\n      \"x\":1980,\n      \"y\":24304\n    },{\n      \"x\":1981,\n      \"y\":23464\n    },{\n      \"x\":1982,\n      \"y\":23708\n    },{\n      \"x\":1983,\n      \"y\":24099\n    },{\n      \"x\":1984,\n      \"y\":24357\n    },{\n      \"x\":1985,\n      \"y\":24237\n    },{\n      \"x\":1986,\n      \"y\":24401\n    },{\n      \"x\":1987,\n      \"y\":24344\n    },{\n      \"x\":1988,\n      \"y\":23586\n    },{\n      \"x\":1989,\n      \"y\":22380\n    },{\n      \"x\":1990,\n      \"y\":21004\n    },{\n      \"x\":1991,\n      \"y\":17287\n    },{\n      \"x\":1992,\n      \"y\":14747\n    },{\n      \"x\":1993,\n      \"y\":13076\n    },{\n      \"x\":1994,\n      \"y\":12555\n    },{\n      \"x\":1995,\n      \"y\":12144\n    },{\n      \"x\":1996,\n      \"y\":11009\n    },{\n      \"x\":1997,\n      \"y\":10950\n    },{\n      \"x\":1998,\n      \"y\":10871\n    },{\n      \"x\":1999,\n      \"y\":10824\n    },{\n      \"x\":2000,\n      \"y\":10577\n    },{\n      \"x\":2001,\n      \"y\":10527\n    },{\n      \"x\":2002,\n      \"y\":10475\n    },{\n      \"x\":2003,\n      \"y\":10421\n    },{\n      \"x\":2004,\n      \"y\":10358\n    },{\n      \"x\":2005,\n      \"y\":10295\n    },{\n      \"x\":2006,\n      \"y\":10104\n    },{\n      \"x\":1970,\n      \"y\":26662\n    },{\n      \"x\":1971,\n      \"y\":26956\n    },{\n      \"x\":1972,\n      \"y\":27912\n    },{\n      \"x\":1973,\n      \"y\":28999\n    },{\n      \"x\":1974,\n      \"y\":28965\n    },{\n      \"x\":1975,\n      \"y\":27826\n    },{\n      \"x\":1976,\n      \"y\":25579\n    },{\n      \"x\":1977,\n      \"y\":25722\n    },{\n      \"x\":1978,\n      \"y\":24826\n    },{\n      \"x\":1979,\n      \"y\":24605\n    },{\n      \"x\":1980,\n      \"y\":24304\n    },{\n      \"x\":1981,\n      \"y\":23464\n    },{\n      \"x\":1982,\n      \"y\":23708\n    },{\n      \"x\":1983,\n      \"y\":24099\n    },{\n      \"x\":1984,\n      \"y\":24357\n    },{\n      \"x\":1985,\n      \"y\":24237\n    },{\n      \"x\":1986,\n      \"y\":24401\n    },{\n      \"x\":1987,\n      \"y\":24344\n    },{\n      \"x\":1988,\n      \"y\":23586\n    },{\n      \"x\":1989,\n      \"y\":22380\n    },{\n      \"x\":1990,\n      \"y\":21004\n    },{\n      \"x\":1991,\n      \"y\":17287\n    },{\n      \"x\":1992,\n      \"y\":14747\n    },{\n      \"x\":1993,\n      \"y\":13076\n    },{\n      \"x\":1994,\n      \"y\":12555\n    },{\n      \"x\":1995,\n      \"y\":12144\n    },{\n      \"x\":1996,\n      \"y\":11009\n    },{\n      \"x\":1997,\n      \"y\":10950\n    },{\n      \"x\":1998,\n      \"y\":10871\n    },{\n      \"x\":1999,\n      \"y\":10824\n    },{\n      \"x\":2000,\n      \"y\":10577\n    },{\n      \"x\":2001,\n      \"y\":10527\n    },{\n      \"x\":2002,\n      \"y\":10475\n    },{\n      \"x\":2003,\n      \"y\":10421\n    },{\n      \"x\":2004,\n      \"y\":10358\n    },{\n      \"x\":2005,\n      \"y\":10295\n    },{\n      \"x\":2006,\n      \"y\":10104\n    }]\n  }]\n}\n\n$(\"#highcharts_fd2d2ee7_06dd_43a4_a45c_724c178f7d8d\").highcharts(data)\n});\n</script>\n"}]},"apps":[],"jobName":"paragraph_1521275024957_416352068","id":"20180317-032344_454862177","dateCreated":"2018-03-17T03:23:44-0500","dateStarted":"2018-03-17T03:44:17-0500","dateFinished":"2018-03-17T03:44:18-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:665"},{"text":"%spark\ninput.addData(USA.drop(30) ++ USSR.drop(30))","user":"anonymous","dateUpdated":"2018-03-17T16:02:32-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res147: org.apache.spark.sql.execution.streaming.Offset = 3\n"}]},"apps":[],"jobName":"paragraph_1521276210119_621491630","id":"20180317-034330_467356825","dateCreated":"2018-03-17T03:43:30-0500","dateStarted":"2018-03-17T03:44:12-0500","dateFinished":"2018-03-17T03:44:13-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:666"},{"title":"Real time","text":"%spark\n\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n// prevent INFO logging from pollution output\nsc.setLogLevel(\"ERROR\")\n\n// creating the StreamingContext with 5 seconds interval\nval ssc = new StreamingContext(sc, Seconds(5))\n\n\nval kafkaConf = Map[String, Object](\n  \"bootstrap.servers\" -> \"localhost:9092\",\n  \"key.deserializer\" -> classOf[StringDeserializer],\n  \"value.deserializer\" -> classOf[StringDeserializer],\n  \"group.id\" -> \"streams-plaintext-input\",\n  \"auto.offset.reset\" -> \"earliest\",\n  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n)\n\n\n// val kafkaParams = Map[String, String](\"metadata.broker.list\" -> \"localhost:9092\")\n //     val topicsSet = \"streams-plaintext-input\".split(\",\").toSet\n //       val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n //       ssc, kafkaParams, topicsSet)\n        \nval topics = Array(\"streams-plaintext-input\")\nval stream = KafkaUtils.createDirectStream[String, String](    \n    ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaConf)\n)\n\nval words1 = stream.map(record => (record.key, record.value))\n\nval words = words1.flatMap{ case(x, y) => y.split(\" \")}\n//\n//lines.foreachRDD { rdd =>\n      //System.out.println(\"--- New RDD with \" + rdd.partitions.size + \" partitions and \" + rdd.count + \" records\")\n //     rdd.foreach { record =>\n //       System.out.print(record.value())\n //     }\n //   }\n\n//val words = lines.flatMap{ case(x, y) => y.split(\" \")}\n\nwords.print()\n\n//val wordCounts = words.toDF() //.groupBy(\"key\").count()\n\nval chart = highcharts(words\n  .series(\"x\" -> \"value\", \"y\" -> $\"count\")\n  //.orderBy($\"count\".desc\")\n  .take(20)).chart(Chart.bar)\nchart.plot()\n\n\n\nssc.start()\n//ssc.awaitTermination() \n","user":"anonymous","dateUpdated":"2018-03-17T16:07:32-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.streaming.kafka010._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.kafka.common.serialization.StringDeserializer\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@5e764b28\nkafkaConf: scala.collection.immutable.Map[String,Object] = Map(key.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer, auto.offset.reset -> earliest, group.id -> streams-plaintext-input, bootstrap.servers -> localhost:9092, enable.auto.commit -> false, value.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer)\ntopics: Array[String] = Array(streams-plaintext-input)\nstream: org.apache.spark.streaming.dstream.InputDStream[org.apache.kafka.clients.consumer.ConsumerRecord[String,String]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@665f39d3\nwords1: org.apache.spark.streaming.dstream.DStream[(String, String)] = org.apache.spark.streaming.dstream.MappedDStream@41066832\nwords: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@183befa2\n<console>:93: error: value series is not a member of org.apache.spark.streaming.dstream.DStream[String]\npossible cause: maybe a semicolon is missing before `value series'?\n         .series(\"x\" -> \"value\", \"y\" -> $\"count\")\n          ^\n"}]},"apps":[],"jobName":"paragraph_1521261148985_-1921116429","id":"20180316-233228_1767936315","dateCreated":"2018-03-16T23:32:28-0500","dateStarted":"2018-03-17T15:13:50-0500","dateFinished":"2018-03-17T15:13:56-0500","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:667"},{"text":"%spark\n\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n// prevent INFO logging from pollution output\nsc.setLogLevel(\"ERROR\")\n\n// creating the StreamingContext with 5 seconds interval\nval ssc = new StreamingContext(sc, Seconds(5))\n\n\nval kafkaConf = Map[String, Object](\n  \"bootstrap.servers\" -> \"localhost:9092\",\n  \"key.deserializer\" -> classOf[StringDeserializer],\n  \"value.deserializer\" -> classOf[StringDeserializer],\n  \"group.id\" -> \"streams-plaintext-input\",\n  \"auto.offset.reset\" -> \"earliest\",\n  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n)\n\n\n// val kafkaParams = Map[String, String](\"metadata.broker.list\" -> \"localhost:9092\")\n //     val topicsSet = \"streams-plaintext-input\".split(\",\").toSet\n //       val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n //       ssc, kafkaParams, topicsSet)\n        \nval topics = Array(\"streams-plaintext-input\")\nval stream = KafkaUtils.createDirectStream[String, String](    \n    ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaConf)\n)\n\nval words = stream.map(record => (record.key, record.value))\n//\n//lines.foreachRDD { rdd =>\n      //System.out.println(\"--- New RDD with \" + rdd.partitions.size + \" partitions and \" + rdd.count + \" records\")\n //     rdd.foreach { record =>\n //       System.out.print(record.value())\n //     }\n //   }\n\n//val words = lines.flatMap{ case(x, y) => y.split(\" \")}\n\nwords.print()\n\nssc.start()\nssc.awaitTermination() \n/*\nval spark = SparkSession\n      .builder\n      .appName(\"tblTweeHashTagStreamReader\")\n      .getOrCreate()\n\n    import spark.implicits._\n\n    // Create DataSet representing the stream of input lines from kafka\n    val lines = spark\n      .readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n      .option(\"subscribe\", \"streams-plaintext-input\")\n      .load()\n      .selectExpr(\"CAST(value AS STRING)\")\n      .as[String]\n\n    // Generate running word count\n    val wordCounts = lines.flatMap(_.split(\" \")).groupBy(\"value\").count()\n\n    // Start running the query that prints the running counts to the console\n    val query = wordCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoints/tblTweetHashTag\")\n      .start()\n\n    query.awaitTermination()*/","user":"anonymous","dateUpdated":"2018-03-17T16:02:46-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521272974256_1692933299","id":"20180317-024934_547574798","dateCreated":"2018-03-17T02:49:34-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:668"},{"text":"%spark\n\n//val words = lines.as[String].flatMap(_.split(\" \"))\n\n// Generate running word count\nval wordCounts = words.groupBy(\"value\").count()\n\nval chart = highcharts(wordCounts\n  .series(\"x\" -> \"value\", \"y\" -> $\"count\"))\n  .orderBy($\"count\".desc\")\n  .take(20)).chart(Chart.bar)\nchart.plot()","user":"anonymous","dateUpdated":"2018-03-17T02:55:48-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521273094453_-1634811367","id":"20180317-025134_1989069934","dateCreated":"2018-03-17T02:51:34-0500","dateStarted":"2018-03-17T02:55:48-0500","dateFinished":"2018-03-17T03:01:27-0500","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:669"},{"text":"%spark\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n/*\n// prevent INFO logging from pollution output\nsc.setLogLevel(\"ERROR\")\n\n// creating the StreamingContext with 5 seconds interval\nval ssc = new StreamingContext(sc, Seconds(5))\n\n\nval kafkaConf = Map[String, Object](\n  \"bootstrap.servers\" -> \"localhost:9092\",\n  \"key.deserializer\" -> classOf[StringDeserializer],\n  \"value.deserializer\" -> classOf[StringDeserializer],\n  \"group.id\" -> \"streams-plaintext-input\",\n  \"auto.offset.reset\" -> \"earliest\",\n  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n)\n\n\n// val kafkaParams = Map[String, String](\"metadata.broker.list\" -> \"localhost:9092\")\n //     val topicsSet = \"streams-plaintext-input\".split(\",\").toSet\n //       val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n //       ssc, kafkaParams, topicsSet)\n        \nval topics = Array(\"streams-plaintext-input\")\nval stream = KafkaUtils.createDirectStream[String, String](    \n    ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaConf)\n)\n\nval words = stream.map(record => (record.key, record.value))\n//\n//lines.foreachRDD { rdd =>\n      //System.out.println(\"--- New RDD with \" + rdd.partitions.size + \" partitions and \" + rdd.count + \" records\")\n //     rdd.foreach { record =>\n //       System.out.print(record.value())\n //     }\n //   }\n\n//val words = lines.flatMap{ case(x, y) => y.split(\" \")}\n\nwords.print()\n// Generate running word count\nval wordCounts = words.groupBy(\"value\").count()\n\nval chart = highcharts(wordCounts\n  .series(\"x\" -> \"value\", \"y\" -> $\"count\"))\n  .orderBy($\"count\".desc\")\n  .take(20)).chart(Chart.bar)\nchart.plot()\n\n\nssc.start()\nssc.awaitTermination() */\n\nval spark = SparkSession\n      .builder\n      .appName(\"tblTweeHashTagStreamReader\")\n      .getOrCreate()\n\n    import spark.implicits._\n\n    // Create DataSet representing the stream of input lines from kafka\n    val lines = spark\n      .readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n      .option(\"subscribe\", \"streams-plaintext-input\")\n      .load()\n      .selectExpr(\"CAST(value AS STRING)\")\n      .as[String]\n\n    // Generate running word count\n    val wordCounts = lines.flatMap(_.split(\" \")).groupBy(\"value\").count()\n    \nval chart = highcharts(wordCounts\n  .series(\"x\" -> \"value\", \"y\" -> $\"count\"))\n  .orderBy($\"count\".desc\")\n  .take(20)).chart(Chart.bar)\nchart.plot()\n\n    // Start running the query that prints the running counts to the console\n    val query = wordCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoints/tblTweetHashTag\")\n      .start()\n\n//    query.awaitTermination()","user":"anonymous","dateUpdated":"2018-03-17T16:02:58-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.streaming.kafka010._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.kafka.common.serialization.StringDeserializer\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6aca846f\nimport spark.implicits._\njava.lang.ClassNotFoundException: Failed to find data source: kafka. Please find packages at http://spark.apache.org/third-party-projects.html\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:569)\n  at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:197)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:87)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:87)\n  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:124)\n  ... 124 elided\nCaused by: java.lang.ClassNotFoundException: kafka.DefaultSource\n  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25$$anonfun$apply$13.apply(DataSource.scala:554)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25$$anonfun$apply$13.apply(DataSource.scala:554)\n  at scala.util.Try$.apply(Try.scala:192)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25.apply(DataSource.scala:554)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$25.apply(DataSource.scala:554)\n  at scala.util.Try.orElse(Try.scala:84)\n  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:554)\n  ... 131 more\n"}]},"apps":[],"jobName":"paragraph_1521272972944_424800925","id":"20180317-024932_1619066461","dateCreated":"2018-03-17T02:49:32-0500","dateStarted":"2018-03-17T03:52:42-0500","dateFinished":"2018-03-17T03:52:52-0500","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:670"},{"text":"%spark\n\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n// prevent INFO logging from pollution output\nsc.setLogLevel(\"ERROR\")\n\n// creating the StreamingContext with 5 seconds interval\nval ssc = new StreamingContext(sc, Seconds(5))\n\n\nval kafkaConf = Map[String, Object](\n  \"bootstrap.servers\" -> \"localhost:9092\",\n  \"key.deserializer\" -> classOf[StringDeserializer],\n  \"value.deserializer\" -> classOf[StringDeserializer],\n  \"group.id\" -> \"streams-plaintext-input\",\n  \"auto.offset.reset\" -> \"earliest\",\n  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n)\n\n\n// val kafkaParams = Map[String, String](\"metadata.broker.list\" -> \"localhost:9092\")\n //     val topicsSet = \"streams-plaintext-input\".split(\",\").toSet\n //       val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n //       ssc, kafkaParams, topicsSet)\n        \nval topics = Array(\"streams-plaintext-input\")\nval stream = KafkaUtils.createDirectStream[String, String](    \n    ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaConf)\n)\n\nval words = stream.map(record => (record.key, record.value))\n//\n//lines.foreachRDD { rdd =>\n      //System.out.println(\"--- New RDD with \" + rdd.partitions.size + \" partitions and \" + rdd.count + \" records\")\n //     rdd.foreach { record =>\n //       System.out.print(record.value())\n //     }\n //   }\n\n//val words = lines.flatMap{ case(x, y) => y.split(\" \")}\n\nwords.print()\n\nssc.start()\nssc.awaitTermination() \n/*\nval spark = SparkSession\n      .builder\n      .appName(\"tblTweeHashTagStreamReader\")\n      .getOrCreate()\n\n    import spark.implicits._\n\n    // Create DataSet representing the stream of input lines from kafka\n    val lines = spark\n      .readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n      .option(\"subscribe\", \"streams-plaintext-input\")\n      .load()\n      .selectExpr(\"CAST(value AS STRING)\")\n      .as[String]\n\n    // Generate running word count\n    val wordCounts = lines.flatMap(_.split(\" \")).groupBy(\"value\").count()\n\n    // Start running the query that prints the running counts to the console\n    val query = wordCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoints/tblTweetHashTag\")\n      .start()\n\n    query.awaitTermination()*/","user":"anonymous","dateUpdated":"2018-03-17T16:03:01-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521268454658_-576966954","id":"20180317-013414_1335763322","dateCreated":"2018-03-17T01:34:14-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:671"},{"text":"%sql\n\nselect col1, col2 from tblTweetHashTag","user":"anonymous","dateUpdated":"2018-03-17T17:08:35-0500","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":true},"lineChart":{"forceY":false,"lineWithFocus":false}},"commonSetting":{},"keys":[{"name":"col1","index":0,"aggr":"sum"}],"groups":[{"name":"col1","index":0,"aggr":"sum"}],"values":[{"name":"col2","index":1,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col1\tcol2\nare\t1\ncommand\t1\ngme\t1\nhello\t2\nhow\t3\ninteresting\t1\nis\t2\nit,\t1\nline\t8\nnot\t1\nthink\t1\nworking\t1\nworld\t3\nyou\t2\n"}]},"apps":[],"jobName":"paragraph_1521175399522_741110100","id":"20180315-234319_781167094","dateCreated":"2018-03-15T23:43:19-0500","dateStarted":"2018-03-17T16:06:56-0500","dateFinished":"2018-03-17T16:06:56-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:672"},{"text":"%spark\n\nimport …   \n\n\nval endpointUrl = \"https://kinesis.us-east-1.amazonaws.com\"\nval credentials = new DefaultAWSCredentialsProviderChain().getCredentials()\n    require(credentials != null,\n      \"No AWS credentials found. Please specify credentials using one of the methods specified \" +\n        \"in http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html\")\n    val kinesisClient = new AmazonKinesisClient(credentials)\n    kinesisClient.setEndpoint(\"https://kinesis.us-east-1.amazonaws.com\")\n    val numShards = kinesisClient.describeStream(\"spark-demo\").getStreamDescription().getShards().size\n\nval numStreams = numShards\n\n//Setting batch interval to 5 seconds\nval batchInterval = Seconds(5)\nval kinesisCheckpointInterval = batchInterval\nval regionName = RegionUtils.getRegionByEndpoint(endpointUrl).getName()\n\n \nval ssc = new StreamingContext(sc, batchInterval)\n\n // Create the DStreams\n    val kinesisStreams = (0 until numStreams).map { i =>\n      KinesisUtils.createStream(ssc, \"app-spark-demo\", \"spark-demo\", endpointUrl, regionName,InitialPositionInStream.LATEST, kinesisCheckpointInterval, StorageLevel.MEMORY_AND_DISK_2)\n    }\n\n\n// Union all the streams\nval unionStreams = ssc.union(kinesisStreams)\n\n//Schema of the incoming data on the stream\nval schemaString = \"device_id,temperature,timestamp\"\n\n//Parse the data in DStreams\nval tableSchema = StructType( schemaString.split(\",\").map(fieldName => StructField(fieldName, StringType, true)))\n\n//Processing each RDD and storing it in temporary table\n unionStreams.foreachRDD ((rdd: RDD[Array[Byte]], time: Time) => {\n  val rowRDD = rdd.map(w => Row.fromSeq(new String(w).split(\",\")))\n  val wordsDF = sqlContext.createDataFrame(rowRDD,tableSchema)\n  wordsDF.registerTempTable(\"realTimeTable\")\n})\n\n","user":"anonymous","dateUpdated":"2018-03-17T16:03:15-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521187476093_-1259947698","id":"20180316-030436_113209093","dateCreated":"2018-03-16T03:04:36-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:673"},{"text":"%hbase\nhelp\n","user":"anonymous","dateUpdated":"2018-03-16T02:53:26-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521184000456_646705863","id":"20180316-020640_805817092","dateCreated":"2018-03-16T02:06:40-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:674"}],"name":"BDTwitrend","id":"2DBDF3FF5","angularObjects":{"2DANCNB5W:shared_process":[],"2DASSW9U5:shared_process":[],"2D7UT1Q6X:shared_process":[],"2D7YBHWUX:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}